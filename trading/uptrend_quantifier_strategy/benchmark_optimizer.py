# -*- coding: utf-8 -*-
"""
å‚æ•°ä¼˜åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·
================================================================================
å¯¹æ¯”ä¸åŒä¼˜åŒ–æ–¹æ³•çš„æ€§èƒ½ï¼š
- ğŸŒ ä¼ ç»Ÿç½‘æ ¼æœç´¢ vs ğŸš€ é«˜æ€§èƒ½ä¼˜åŒ–
- ğŸ“Š æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”å’Œå¯è§†åŒ–
- â±ï¸ è¯¦ç»†çš„æ—¶é—´åˆ†æ
- ğŸ¯ ä¼˜åŒ–æ•ˆæœè¯„ä¼°
================================================================================
"""

import time
import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

# è®¾ç½®è·¯å¾„
script_path = os.path.abspath(__file__)
project_root = os.path.dirname(os.path.dirname(os.path.dirname(script_path)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)


class OptimizationBenchmark:
    """
    å‚æ•°ä¼˜åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•
    """
    
    def __init__(self, strategy_class, data_dir: str, results_dir: str):
        self.strategy_class = strategy_class
        self.data_dir = data_dir
        self.results_dir = results_dir
        self.benchmark_results = {}
        
        # åˆ›å»ºåŸºå‡†æµ‹è¯•ç›®å½•
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.benchmark_dir = os.path.join(results_dir, f"benchmark_{timestamp}")
        os.makedirs(self.benchmark_dir, exist_ok=True)
    
    def run_traditional_optimization(self, param_grid: Dict) -> Tuple[Dict, float]:
        """
        è¿è¡Œä¼ ç»Ÿç½‘æ ¼æœç´¢
        """
        print("ğŸŒ è¿è¡Œä¼ ç»Ÿç½‘æ ¼æœç´¢...")
        start_time = time.time()
        
        # ä½¿ç”¨åŸå§‹çš„å‚æ•°ä¼˜åŒ–æ–¹æ³•
        try:
            from trading.param_opt import run_parameter_optimization
            
            # ä¸´æ—¶ç›®å½•ç”¨äºä¼ ç»Ÿä¼˜åŒ–
            traditional_dir = os.path.join(self.benchmark_dir, "traditional")
            os.makedirs(traditional_dir, exist_ok=True)
            
            # è¿è¡Œä¼ ç»Ÿä¼˜åŒ–
            run_parameter_optimization(
                strategy_class=self.strategy_class,
                param_grid=param_grid,
                data_dir=self.data_dir,
                results_dir=traditional_dir,
                start_date='2021-01-01',
                end_date='2025-07-08'
            )
            
            traditional_time = time.time() - start_time
            
            # è¯»å–ç»“æœ
            result_files = [f for f in os.listdir(traditional_dir) if f.startswith('optimization_summary')]
            if result_files:
                latest_file = max(result_files)
                results_df = pd.read_csv(os.path.join(traditional_dir, latest_file))
                best_result = results_df.iloc[0].to_dict()
            else:
                best_result = {'score': 0}
            
            return best_result, traditional_time
            
        except Exception as e:
            print(f"âŒ ä¼ ç»Ÿä¼˜åŒ–å¤±è´¥: {e}")
            return {'score': 0}, time.time() - start_time
    
    def run_advanced_optimization(self, param_ranges: Dict) -> Tuple[Dict, float]:
        """
        è¿è¡Œé«˜æ€§èƒ½ä¼˜åŒ–
        """
        print("ğŸš€ è¿è¡Œé«˜æ€§èƒ½ä¼˜åŒ–...")
        start_time = time.time()
        
        try:
            from trading.uptrend_quantifier_strategy.advanced_param_optimizer import AdvancedParameterOptimizer
            
            # åˆ›å»ºé«˜æ€§èƒ½ä¼˜åŒ–å™¨
            optimizer = AdvancedParameterOptimizer(
                strategy_class=self.strategy_class,
                data_dir=self.data_dir,
                results_dir=os.path.join(self.benchmark_dir, "advanced"),
                start_date='2021-01-01',
                end_date='2025-07-08'
            )
            
            # è¿è¡Œä¼˜åŒ–
            results_df = optimizer.optimize(
                param_ranges=param_ranges,
                optimization_method='hybrid',
                n_initial=15,  # å‡å°‘æµ‹è¯•æ—¶é—´
                n_bayesian=20,
                n_refined=10
            )
            
            advanced_time = time.time() - start_time
            
            if not results_df.empty:
                best_result = results_df.iloc[0].to_dict()
            else:
                best_result = {'score': 0}
            
            return best_result, advanced_time
            
        except Exception as e:
            print(f"âŒ é«˜æ€§èƒ½ä¼˜åŒ–å¤±è´¥: {e}")
            return {'score': 0}, time.time() - start_time
    
    def run_benchmark(self) -> Dict:
        """
        è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•
        """
        print("ğŸ¯ å¯åŠ¨å‚æ•°ä¼˜åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 80)
        
        # å®šä¹‰æµ‹è¯•å‚æ•°
        param_grid = {
            'len_short': range(15, 26, 5),
            'len_mid': range(45, 56, 5), 
            'len_long': range(180, 201, 10),
            'adx_len': range(12, 17, 2),
            'adx_threshold': range(23, 28, 2),
        }
        
        param_ranges = {
            'len_short': range(15, 26, 2),
            'len_mid': range(45, 56, 2),
            'len_long': range(180, 201, 5),
            'adx_len': range(12, 17, 1),
            'adx_threshold': range(23, 28, 1),
        }
        
        print("ğŸ“Š æµ‹è¯•å‚æ•°ç»„åˆæ•°:")
        traditional_combinations = 1
        for param, values in param_grid.items():
            count = len(list(values))
            traditional_combinations *= count
            print(f"  {param}: {count} ä¸ªå€¼")
        print(f"  ä¼ ç»Ÿæ–¹æ³•æ€»ç»„åˆ: {traditional_combinations}")
        
        advanced_max_tests = 15 + 20 + 10  # initial + bayesian + refined
        print(f"  é«˜æ€§èƒ½æ–¹æ³•æœ€å¤§æµ‹è¯•: {advanced_max_tests}")
        print(f"  ç†è®ºåŠ é€Ÿæ¯”: {traditional_combinations / advanced_max_tests:.1f}x")
        
        # è¿è¡Œä¼ ç»Ÿä¼˜åŒ–
        print("\n" + "="*50)
        traditional_result, traditional_time = self.run_traditional_optimization(param_grid)
        
        # è¿è¡Œé«˜æ€§èƒ½ä¼˜åŒ–  
        print("\n" + "="*50)
        advanced_result, advanced_time = self.run_advanced_optimization(param_ranges)
        
        # æ±‡æ€»ç»“æœ
        benchmark_summary = {
            'traditional': {
                'time': traditional_time,
                'best_score': traditional_result.get('score', 0),
                'combinations_tested': traditional_combinations,
                'method': 'ç½‘æ ¼æœç´¢'
            },
            'advanced': {
                'time': advanced_time,
                'best_score': advanced_result.get('score', 0),
                'combinations_tested': advanced_max_tests,
                'method': 'æ··åˆæ™ºèƒ½ä¼˜åŒ–'
            }
        }
        
        # è®¡ç®—æ€§èƒ½æå‡
        if traditional_time > 0:
            time_speedup = traditional_time / advanced_time
            efficiency_ratio = advanced_max_tests / traditional_combinations
            
            benchmark_summary['performance'] = {
                'time_speedup': time_speedup,
                'efficiency_ratio': efficiency_ratio,
                'score_comparison': advanced_result.get('score', 0) - traditional_result.get('score', 0)
            }
        
        # ä¿å­˜å’Œæ˜¾ç¤ºç»“æœ
        self._save_benchmark_results(benchmark_summary)
        self._print_benchmark_report(benchmark_summary)
        
        return benchmark_summary
    
    def _save_benchmark_results(self, results: Dict):
        """
        ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ
        """
        import json
        
        # ä¿å­˜JSONç»“æœ
        results_path = os.path.join(self.benchmark_dir, 'benchmark_results.json')
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆæŠ¥å‘Š
        report_path = os.path.join(self.benchmark_dir, 'benchmark_report.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("ğŸ¯ å‚æ•°ä¼˜åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š\n")
            f.write("=" * 80 + "\n\n")
            
            f.write("ğŸ“Š æµ‹è¯•ç»“æœå¯¹æ¯”:\n")
            f.write("-" * 50 + "\n")
            
            for method, data in results.items():
                if method == 'performance':
                    continue
                f.write(f"\n{data['method']}:\n")
                f.write(f"  â±ï¸  æ‰§è¡Œæ—¶é—´: {data['time']:.2f} ç§’\n")
                f.write(f"  ğŸ¯ æœ€ä½³å¾—åˆ†: {data['best_score']:.4f}\n")
                f.write(f"  ğŸ”¢ æµ‹è¯•ç»„åˆæ•°: {data['combinations_tested']}\n")
            
            if 'performance' in results:
                perf = results['performance']
                f.write(f"\nğŸš€ æ€§èƒ½æå‡:\n")
                f.write("-" * 30 + "\n")
                f.write(f"  âš¡ æ—¶é—´åŠ é€Ÿ: {perf['time_speedup']:.1f}x\n")
                f.write(f"  ğŸ“ˆ æ•ˆç‡æå‡: {1/perf['efficiency_ratio']:.1f}x\n")
                f.write(f"  ğŸ¯ å¾—åˆ†å·®å¼‚: {perf['score_comparison']:+.4f}\n")
    
    def _print_benchmark_report(self, results: Dict):
        """
        æ‰“å°åŸºå‡†æµ‹è¯•æŠ¥å‘Š
        """
        print("\n" + "=" * 80)
        print("ğŸ¯ å‚æ•°ä¼˜åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        print("=" * 80)
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        comparison_data = []
        for method, data in results.items():
            if method == 'performance':
                continue
            comparison_data.append({
                'ä¼˜åŒ–æ–¹æ³•': data['method'],
                'æ‰§è¡Œæ—¶é—´(ç§’)': f"{data['time']:.2f}",
                'æœ€ä½³å¾—åˆ†': f"{data['best_score']:.4f}",
                'æµ‹è¯•ç»„åˆæ•°': data['combinations_tested']
            })
        
        df = pd.DataFrame(comparison_data)
        print("\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
        print(df.to_string(index=False))
        
        if 'performance' in results:
            perf = results['performance']
            print(f"\nğŸš€ æ€§èƒ½æå‡åˆ†æ:")
            print("-" * 50)
            print(f"âš¡ æ‰§è¡Œæ—¶é—´åŠ é€Ÿ:     {perf['time_speedup']:.1f}x")
            print(f"ğŸ“ˆ æœç´¢æ•ˆç‡æå‡:     {1/perf['efficiency_ratio']:.1f}x") 
            print(f"ğŸ¯ æœ€ä½³å¾—åˆ†å·®å¼‚:     {perf['score_comparison']:+.4f}")
            
            if perf['time_speedup'] > 1:
                print(f"\nâœ… é«˜æ€§èƒ½ä¼˜åŒ–åœ¨ {perf['time_speedup']:.1f}x çš„é€Ÿåº¦ä¸‹")
                if perf['score_comparison'] >= 0:
                    print("   è·å¾—äº†ç›¸åŒæˆ–æ›´å¥½çš„ä¼˜åŒ–ç»“æœï¼")
                else:
                    print("   å¾—åˆ†ç•¥æœ‰ä¸‹é™ï¼Œä½†ä»åœ¨å¯æ¥å—èŒƒå›´å†…ã€‚")
            
        print(f"\nğŸ“ è¯¦ç»†ç»“æœä¿å­˜åœ¨: {self.benchmark_dir}")
        print("=" * 80)


def run_optimization_benchmark():
    """
    è¿è¡Œä¼˜åŒ–åŸºå‡†æµ‹è¯•
    """
    # è®¾ç½®è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(script_dir, '..', 'tushare_data', 'daily')
    results_dir = os.path.join(script_dir, '..', 'results', 'benchmark')
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(results_dir, exist_ok=True)
    
    # å¯¼å…¥ç­–ç•¥
    from trading.uptrend_quantifier_strategy.uptrend_quantifier_strategy import UptrendQuantifierStrategy
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å™¨
    benchmark = OptimizationBenchmark(
        strategy_class=UptrendQuantifierStrategy,
        data_dir=data_dir,
        results_dir=results_dir
    )
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    results = benchmark.run_benchmark()
    
    print("\nğŸ’¡ åŸºå‡†æµ‹è¯•è¦ç‚¹:")
    print("- ğŸ§  æ™ºèƒ½ä¼˜åŒ–å‡å°‘äº†æ— æ•ˆå‚æ•°ç»„åˆçš„æµ‹è¯•")
    print("- âš¡ å¤šå±‚ä¼˜åŒ–ç­–ç•¥æå‡æœç´¢æ•ˆç‡")
    print("- ğŸ¯ è´å¶æ–¯ä¼˜åŒ–èƒ½æ›´å¿«æ‰¾åˆ°æœ€ä¼˜åŒºåŸŸ")
    print("- ğŸ’¾ ç¼“å­˜æœºåˆ¶é¿å…é‡å¤è®¡ç®—")
    print("- ğŸ›¡ï¸ æ—©åœç­–ç•¥èŠ‚çœè®¡ç®—èµ„æº")
    
    return results


if __name__ == "__main__":
    results = run_optimization_benchmark() 